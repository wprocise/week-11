{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8adbb63",
   "metadata": {},
   "source": [
    "# Announcements\n",
    "\n",
    "- MVP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9024fc9e-f17d-4f15-b174-1f268d32f6af",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "In the Supervised Learning paradigm, we have a collection of observations (our data), each with a set of features and a particular variable of interest, i.e., the \"target\" (in regression) or \"label\" (in classification) which we aim to predict.\n",
    "\n",
    "In the **Unsupervised Learning** paradigm, there is no particular variable of interest. In this way, the goal is not to predict an aspect of, but rather **to organize the data**, and subsequently act on our organization. Most forms of unsupervised learning can be categorized as one of the following:\n",
    "\n",
    "- **Dimensionality Reduction.** Use relationships between rows and columns to represent the data with fewer columns.\n",
    "- **Clustering:** Group rows of data into categories based on their featuers (\"similar\" data are placed into groups).\n",
    "- **Recommendation.** Use the above two strategies to acertain what might be a good recommendation to someone with access to the features of data in question.\n",
    "\n",
    "And, all three of these are based on the notion of \"**similarity**\".\n",
    "\n",
    "*With unsupervised learning, we will mainly use the `.transform()` and `.fit_transform()` methods in Scikit-Learn that we saw in the lab on NLP methods.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a66fa9-578b-4a24-8b22-85d3ab3228a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_theme(context='notebook', style='whitegrid', font_scale=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d00ffb4-e1ed-4c13-a95a-1d0b25998aeb",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "Unsupervised Learning is still a form of \"machine\" learning, so we grant that our work is meant to generalize to data that is yet unseen. So, we should still have a hold out (or \"test\") set of data on which to evaluate our models later.\n",
    "\n",
    "In this lab, we'll be using a collection of beer review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fcab22-312e-467f-baaa-d39a58bc7171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf63b84-6c13-4cd8-995b-cbfec7beb06b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can use `read_csv` to read the compressed csv file\n",
    "url = \"https://raw.githubusercontent.com/leontoddjohnson/datasets/main/data/beer_reviews/beer_reviews.csv.zip\"\n",
    "df_raw = pd.read_csv(url, compression='zip', low_memory=False)\n",
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc91613-8b18-4b0c-8ec1-cd454a0ea063",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_beer_reviews, df_holdout = train_test_split(df_raw, test_size=0.20, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b8223-fe6a-426e-8c8a-5d6f9ba84f8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_beer_reviews.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3559aaef-7a0d-40f6-adb0-dfb9922d3c9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# there are not many NA values compared to all the data, so we drop them\n",
    "df_beer_reviews.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f43449-77fa-4f6f-96be-03f6b3fb8eb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_beer_reviews.to_csv('./data/beer_reviews.csv', index=False)\n",
    "# df_holdout.to_csv('./data/beer_reviews_holdout.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0675d33-541c-4792-a293-343257a00b69",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality\n",
    "\n",
    "As data sets become increasingly wide (i.e., more columns), they in turn become more unweildy. We may want as many columns as possible to maximize the information we have for each observation, but as we do so, we run into a couple of problems:\n",
    "\n",
    "1. more columns will increase the computational load on any algorithm acting on that data. \n",
    "2. high-dimensional spaces quickly become sparse, and require exponentially more data accurately represent a situation.\n",
    "\n",
    "This phenomenon is commonly called **\"The Curse of Dimensionality\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce17e61a-33e4-4264-bf55-1754c84c9b00",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "We can combat the curse of dimensionality using dimensionality reduction techniques. In dimensionality reduction, we extract latent features in our data that otherwise would go unnoticed (a.k.a., \"Feature Extraction\"). These latent features retain some acceptable percentage of the information from the original dataset, and we can use a fewer number of them in lieu of the wider dataset we started with. *Note: In a way, classification and regression are extreme forms of dimensionality reduction.*\n",
    "\n",
    "One of the most common dimensionality reduction techniques is one we've already seen: Principal Components Analysis (PCA) which uses Singular Value Decomposition (SVD). Recall that SVD is a guaranteed decomposition for any rectangular matrix. It creates the following equality:\n",
    "\n",
    "$$\n",
    "X = U\\Sigma V^\\top\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28348985-85f2-47ab-bb21-0c24a4627894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d7d375-4241-4b2d-bf9b-2f856299d94e",
   "metadata": {},
   "source": [
    "### Indexing Entities\n",
    "\n",
    "We'll start by only considering beer-related columns of data by aggregating the reviews across users. So, if multiple users reviewed a beer, we only need the average review rating(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1fe0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = ['review_overall', 'review_aroma',\n",
    "                   'review_appearance', 'review_palate', \n",
    "                   'review_taste', 'beer_abv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df37bd34-a125-48e0-83b2-df9e602ee980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_beers = (\n",
    "    df_beer_reviews\n",
    "        .groupby(['beer_name', 'beer_beerid', 'brewery_name', 'beer_style'])\n",
    "        [numeric_columns].mean()\n",
    "        .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4743f49b-57ef-41e1-a18b-dc64ce13b2b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_beers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d9098b-0c7b-4777-8a08-e4220b21f737",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Number of `beer_beerids`: \", df_beers['beer_beerid'].nunique())\n",
    "print(\"Number of `beer_names`: \", df_beers['beer_name'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00cd3ff-8bd0-4f15-8392-9e053b9b069d",
   "metadata": {},
   "source": [
    "The \"names\" don't quite match up to their IDs, so we create a unique identifier for each row, and reassign it as the index for the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f506eee8-c836-4205-9ba4-b4e68a7144aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_beers['beer_id'] = df_beers['beer_name'] + ' (' + df_beers['beer_beerid'].astype(str) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0018cb5e-2c07-477e-bd68-ef0275b9efed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_beers['beer_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39c72d8-1c32-4d4d-8455-fd4551b441b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_beers.set_index('beer_id', inplace=True)\n",
    "df_beers.drop(columns=['beer_beerid', 'beer_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63498b45-cca5-4788-b79d-e493fd9089b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_beers.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eb46ca-9459-4cea-b7da-d4d594ab6bbd",
   "metadata": {},
   "source": [
    "Next, we'll reduce our scope to only the top 100 breweries, and the top 100 beer styles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7457eaa9-625d-412a-8fe4-8c0dd99f0872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's just look at the top breweries and the top beer styles\n",
    "n_breweries = 100\n",
    "n_beer_styles = 100\n",
    "\n",
    "# `.value_counts` sorts the series descendingly by default\n",
    "brewery_popularity = df_beers.brewery_name.value_counts()\n",
    "top_breweries = brewery_popularity.iloc[:n_breweries].index\n",
    "\n",
    "beer_style_popularity = df_beers.beer_style.value_counts()\n",
    "top_beer_styles = beer_style_popularity.iloc[:n_beer_styles].index\n",
    "\n",
    "df_beers_top = df_beers[(df_beers.brewery_name.isin(top_breweries)) &\n",
    "                        (df_beers.beer_style.isin(top_beer_styles))].copy()\n",
    "\n",
    "df_beers_top.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec412a75-f50e-43e3-8e5a-4d6f1ea8df95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_beers_top.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bd9534",
   "metadata": {},
   "source": [
    "### Gathering Numerical Data\n",
    "\n",
    "To get the most out of this data, we need to convert the `brewery_name` and `beer_style` to numerical values. We could use pandas categories, and convert these values to labels (e.g., `\"Altbier\" --> 1` and `\"Tripel\" --> 2`), but just because two label numbers are close together (e.g., 1 and 2) doesn't mean that the objects behind those numbers (e.g., Altbiers and Tripels) are so similar. So, to retain a sense for vector \"similarity\", we use one-hot encoding, or dummy variables.\n",
    "\n",
    "*Note: if there were an order to the text values, such as `\"Low\" - \"Med\" - \"High\"`, then category labels might make more sense. But, that's not the case here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15c3ad5-4ff0-488d-ba3d-1da725abf59a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_beers_num = pd.get_dummies(df_beers_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35110d55-da97-4aa0-903f-780aaf8d04f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_beers_num.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b9cc03-f1d5-46b3-a318-400cb760840f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# proportion of the data which is zero\n",
    "(df_beers_num == 0).sum().sum() / (df_beers_num.shape[0] * df_beers_num.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f887b97-46d3-401f-8cf2-f00af25110a1",
   "metadata": {},
   "source": [
    "This is a pretty wide *and sparse* data frame, and all of these columns may slow down any algorithms we plan to use on it. So, we use PCA to reduce the dimensionality. PCA decomposes the data into the following matrices: $U$, $\\Sigma$, $V^\\top$. In our case:\n",
    "\n",
    "- The columns of $V$ (rows of $V^\\top$) are called \"principal components\", and each one is a combination of the features of $X$ (call these \"beer-review flavors\").\n",
    "- $U$ represents each beer in terms of \"beer-review flavor\" strengths. Each beer-review flavor is a sort of mixture of the beer's brewery, beer style, ABV, and review overall. In some beer-review flavors, brewery plays more of a role than the others, in some flavors the ABV plays more of a role, and so on. The first (column) of these maintains the most variance or diversity.\n",
    "- $\\Sigma$ is a diagonal matrix (typically represented as a list of values), where each value represents the amount of variance \"explained\" by the corresponding beer-review flavor. *These values are typically presented in descending order.*\n",
    "\n",
    "\\**Note: in dimensionality reduction, we are typically only concerned with $U$ and $\\Sigma$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fc0d8b-8227-42bf-a3e6-501fdf985b13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "\n",
    "beers_pca_all = pca.fit_transform(df_beers_num)          # this is U\n",
    "exp_variances = pca.explained_variance_ratio_            # this is Sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0746b0e-a5ce-4795-bc17-2b1e94700459",
   "metadata": {},
   "source": [
    "But, we do not need all columns of $U$. We only need enough to capture a sufficient amount of variance explained in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eb845a-4f4a-4de4-a464-7a54bfaa631d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot explained variance\n",
    "def plot_pca(exp_var_ratios, threshold = 0.85):\n",
    "    exp_var_cumsum = exp_var_ratios.cumsum()\n",
    "    t = (exp_var_cumsum < threshold).sum()\n",
    "    g = sns.lineplot(exp_var_cumsum)\n",
    "    plt.axvline(x=t+1, ls='--', color='gray', label=f\"> 85% ({t+1} components)\")\n",
    "\n",
    "    g.set_title(\"Variance Explained after PCA\")\n",
    "    g.set_xlabel(\"Principal Component\")\n",
    "    g.set_ylabel(\"Cumulative Explained Variance\")\n",
    "    g.legend();\n",
    "    \n",
    "    return g\n",
    "\n",
    "g = plot_pca(exp_variances, threshold = 0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ff8ae0-2219-4673-a324-78d98dc7dd16",
   "metadata": {},
   "source": [
    "Suppose we are comfortable with a model which uses 85% of the variance in our data. Then we are comfortable saying the remaining 15% of the variation in beer reviews is due to either (a) random noise or (b) beer review \"flavors\" that are so subtle they are negligible. In this case, we only need to keep 28 components. This is a significant improvement on the original 200+ columns of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24d6be8-7591-464a-978b-249c9d4d3dad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "beers_pca = beers_pca_all[:, :28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043c5d26-097b-4f62-8082-d85950fd7a0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# these correspond to the rows of `df_beers_top`\n",
    "df_beers_pca = pd.DataFrame(beers_pca, index=df_beers_top.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cdf3f7-dfdd-40bb-934f-d5df6d82fb05",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Similarity\n",
    "\n",
    "A foundational element of unsupervised learning is the notion of \"similarity.\" The way(s) in which this idea is defined will direct the rest of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca568b9-5102-406b-bf32-c9d25f0472c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a8939d-d27e-4f6e-b50c-46c5c48996d7",
   "metadata": {},
   "source": [
    "There are several similarity (or distance) metrics, but in this lab we'll investigate a few of the most common ones:\n",
    "\n",
    "- Euclidean Distance\n",
    "- Cosine Distance\n",
    "- Jaccard Distance\n",
    "- Manhattan Distance\n",
    "\n",
    "Note: \"Similarity\" is the complement of \"Distance\". Typically, similarity metrics exist between 0 and 1, so in general, $\\text{distance} = 1 - \\text{similarity}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773ef2e7-ab53-4c4f-baed-2dc71bf56bf4",
   "metadata": {},
   "source": [
    "### Euclidean Distance\n",
    "\n",
    "The Euclidean Distance represents **the length of the line which connects two points in a vector space**. For two points on the x-y plane, it is the length of the hypotenuse of the triangle formed by the two points and a right angle between them. In general, for two points $p$ and $q$ in $k$-dimensional data, we have\n",
    "\n",
    "$$\n",
    "d_E(\\mathbf{p}, \\mathbf{q})={\\sqrt {(p_{1}-q_{1})^{2}+(p_{2}-q_{2})^{2}+\\cdots +(p_{k}-q_{k})^{2}}}\n",
    "$$\n",
    "\n",
    "*You can think of $\\mathbf{p}$ and $\\mathbf{q}$ as two rows of data, for instance.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772668fa-d5bb-4a52-822c-0657d14b4dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# three points on the 2-D Euclidean plane\n",
    "points = [(1, 1),\n",
    "          (1, 2),\n",
    "          (4, 2)]\n",
    "\n",
    "dists = pairwise_distances(points, metric='euclidean', n_jobs=-1)\n",
    "df_dists = pd.DataFrame(data=dists.round(2), index=points, columns=points)\n",
    "df_dists\n",
    "# g = sns.heatmap(df_dists, annot=True, cmap='Blues');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760b2522-7146-4ba3-96ee-e4d35035cac4",
   "metadata": {},
   "source": [
    "### Cosine Distance\n",
    "\n",
    "For two points (e.g., rows of data) in the **first quadrant** (i.e., all non-negative values), the Cosine Similarity is **the cosine of the angle between two vectors**. The Cosine *Distance* is 1 minus this value. In general, for two points in $k$-dimensional space, this is\n",
    "\n",
    "$$\n",
    "d_C(\\mathbf {p}, \\mathbf {q})=1 - \\cos(\\theta )=1 - {\\mathbf {p} \\cdot \\mathbf {q}  \\over \\|\\mathbf {p} \\|\\|\\mathbf {q} \\|}=1 - {\\frac {\\sum \\limits _{i=1}^{n}{p_{i}q_{i}}}{{\\sqrt {\\sum \\limits _{i=1}^{n}{p_{i}^{2}}}}\\cdot {\\sqrt {\\sum \\limits _{i=1}^{n}{q_{i}^{2}}}}}}\n",
    "$$\n",
    "\n",
    "In fact, this is the same thing as 1 minus the (Pearson's) correlation coefficient between $\\mathbf{p} = \\mathbf{x_1} - \\overline{\\mathbf{x}}_1$ and $\\mathbf{q} = \\mathbf{x_2} - \\overline{\\mathbf{x}}_2$ for some vectors $\\mathbf{x}$ and $\\mathbf{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af1393c-7c86-4788-9fc7-e70fe4f52548",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [(1, 1),\n",
    "          (1, 2),\n",
    "          (2, 4)]\n",
    "\n",
    "dists = pairwise_distances(points, metric='cosine', n_jobs=-1)\n",
    "pd.DataFrame(data=dists.round(2), index=points, columns=points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d6e4fe-6fb1-4daa-b348-1c70122c5f1d",
   "metadata": {},
   "source": [
    "### Jaccard Distance\n",
    "\n",
    "The [Jaccard Similarity](https://en.wikipedia.org/wiki/Jaccard_index) is a consideration of two vectors as if they were sets of items. It represents **the proportion of elements shared between two sets of data.** The more items they share (e.g., maybe each column represents a binary indicator of an item), the higher the Jaccard Similarity. *Note: This measure makes the most sense for binary data.* The Jaccard *Distance* is 1 minus this value:\n",
    "\n",
    "$$\n",
    "d_J(\\mathbf{p}, \\mathbf{q}) = 1 - {|\\mathbf{p} \\cap \\mathbf{q}| \\over |\\mathbf{p} \\cup \\mathbf{q}|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6cabe5-a930-4300-9e43-ce700df84bae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "points_tuples = [(0, 0, 1),\n",
    "                 (1, 1, 0),\n",
    "                 (1, 1, 1)]\n",
    "\n",
    "# in sklearn, jaccard distance requires boolean data\n",
    "points = np.array(points_tuples).astype(bool)\n",
    "\n",
    "dists = pairwise_distances(points, metric='jaccard')\n",
    "pd.DataFrame(data=dists.round(2), index=points_tuples, columns=points_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d2d22d-c4b7-4483-bc9c-d8db2b535367",
   "metadata": {},
   "source": [
    "### Manhattan Distance\n",
    "\n",
    "Manhattan Distance, or Taxicab Distance is **the sum of the perpendicular distances along the axes of a vector space, between two points.** So, in two dimensions, it is the total \"horizontal\" + \"vertical\" distance between two points.\n",
    "\n",
    "$$\n",
    "{\\displaystyle d_{\\text{T}}(\\mathbf {p} ,\\mathbf {q} )=\\sum _{i=1}^{n}\\left|p_{i}-q_{i}\\right|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca3fe2d-45e6-4fce-83c6-ac665e31f348",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "points = [(1, 1),\n",
    "          (1, 2),\n",
    "          (4, 2)]\n",
    "\n",
    "dists = pairwise_distances(points, metric='manhattan', n_jobs=-1)\n",
    "pd.DataFrame(data=dists.round(2), index=points, columns=points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9b210e-a5c2-4740-8a2c-cc1df898d18d",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Suppose we had 100 rows of data with two columns: `purchases_per_month` and `spend_per_month`. Consider the following, for a possible scenario. *Note: this is generated data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98625435-4008-435b-a6f4-032142f5b76e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "centers = [(5, 5), (10, 10)]\n",
    "\n",
    "blobs, blob_labels = make_blobs(n_samples=100, n_features=2, cluster_std=1.0,\n",
    "                                centers=centers, shuffle=False, random_state=42)\n",
    "\n",
    "g = sns.scatterplot(x=blobs[:, 0], y=blobs[:, 1], hue=blob_labels, \n",
    "                    palette='Dark2')\n",
    "sns.scatterplot(x=[10], y=[6], label='noise?', color='black')\n",
    "\n",
    "g.set_xlabel('$ Spent')\n",
    "g.set_ylabel('Purchases per Month')\n",
    "g.set_title(\"(Made Up) Monthly Customers\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5e092a-9701-4983-a2ae-1de309d37c51",
   "metadata": {},
   "source": [
    "Visually, we can divide these data into two groups based on their features. Maybe the bottom group represents \"frugal\" customers, and the top group could be \"lavish\" customers. But, computationally, the question is a bit more difficult, *especially* when it comes to data with more than three columns.\n",
    "\n",
    "The purpose of clustering is to use the features of our data to computationally assign each observation to a group, based on some kind of similarity metric. Technically, the term **clustering** refers to grouping data in such a way there could be points leftover; we may call these points \"noise\". **Partitioning** on the other hand, partitions the data in such a way that *every* point is assigned to one partition. We typically use the term \"clustering\" to accont for both of these.\n",
    "\n",
    "In this lab, we will cover the k-Means algorithm, which is still one of the most scalable and interpretable algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a470a7f4-bd57-4115-b33e-6411283a44bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f5d295-3410-41a5-ac7c-403208ecd401",
   "metadata": {},
   "source": [
    "### k-Means\n",
    "\n",
    "k-Means clustering is a **partitioning** algorithm that divides data into $k$ clusters. Points are assigned to a cluster based on similarity to nearest cluster centroid. The value of $k$ is chosen by the user as a hyperparameter for the algorithm.\n",
    "\n",
    "    1. Choose `k` centroids (e.g., randomly)\n",
    "    2. Assign points to cluster based on nearest centroid\n",
    "    3. Recompute centroids based on each cluster's \"average\" point\n",
    "    4. Repeat steps (2) and (3) until algorithm converges\n",
    "\n",
    "*Interestingly, the boundary regions for KNN can always be reduced to a specific [Vonoroi Diagram](https://en.wikipedia.org/wiki/Voronoi_diagram).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0dbb4c-7be0-4226-831c-ffb9db355e01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# suppose we start with 5 clusters (arbitrary at the start)\n",
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters, n_init='auto')\n",
    "km.fit(df_beers_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a15e7ba-296b-4d27-a26b-1b24129ebb8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_beers_top['cluster_kmeans'] = km.labels_.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0be31bc-263d-4142-ae93-0d59ac44cdf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_beers_top[['beer_style', 'cluster_kmeans']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd0d92f-c15c-42c8-b0f3-2caccf46e49b",
   "metadata": {},
   "source": [
    "**Strengths:**\n",
    "1. Simple parameter ($k$ clusters)\n",
    "2. Relatively fast for $n$ points in $d$-dimensions. The runtime is $O(nkdi)$ where $i$ is number of iterations until convergence.\n",
    "3. Guaranteed to converge.\n",
    "4. Easy to implement.\n",
    "\n",
    "**Weaknesses:**\n",
    "1. Optimal $k$ is often not obvious.\n",
    "2. Can get trapped in local minima (initial conditions matter).\n",
    "3. Sensitive to outliers (partitioning not clustering).\n",
    "4. Scaling affects results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cf7e53-9393-4e40-b74d-64c8393825dc",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "There are many different ways to evaluate a clustering algorithm, but here we'll just discuss the following methods:\n",
    "\n",
    "- Inertia\n",
    "- Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d3c530-b3af-437e-8b4e-2bca4d466654",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Inertia\n",
    "\n",
    "Inertia roughly translates to \"good clustering = points are close to cluster centroids\". In essence, we calculate the (squared) Euclidean Distance between each point and their closest centroids, add them up, and call this \"inertia\". In fact, minimizing this is the main goal of the k-Means algorithm.\n",
    "\n",
    "$$\n",
    "I = \\sum_{i = 0}^n\\min_{\\bar{x}_j\\in C}\\left(\\|x_i - \\bar{x}_j\\|^2\\right)\n",
    "$$\n",
    "\n",
    "where $x_i$ is each point in the data, $\\bar{x}_j$ represents the \"average\" centroid point of cluster $j$, and $C$ is the collection of all clusters. We want $I$ to be as small as possible. *Note: $I \\geq 0$, and $I = 0$ only when all points in a cluster lie at the same location, or the number of clusters = number of points.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bf4e2d-485f-4f1d-b500-216aa4a43d45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "km.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd98462b-46e2-4908-a86c-41fecd1283b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inertias = []\n",
    "\n",
    "for num_clusters in range(2, 12):\n",
    "    km = KMeans(n_clusters=num_clusters, n_init='auto')\n",
    "    km.fit(df_beers_pca)\n",
    "    \n",
    "    inertias.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d3cd86-0625-4f26-94ef-b91e24a90682",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = sns.lineplot(x=range(2, 12), y=inertias)\n",
    "g.set_xlabel('Number of Clusters')\n",
    "g.set_ylabel(\"Intertia\")\n",
    "g.set_title(\"k-Means Intertia over k Clusters\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1400775e-02f9-4f18-9ca8-4f68ac6991f5",
   "metadata": {},
   "source": [
    "A good rule of thumb for picking the best value in a situation like this is to use the [\"elbow\" method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)). Essentially, this is the point at which the inertia decreases with diminishing returns. In our case, it looks like our selection of $k=5$ clusters is best for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29061483-0492-4da4-8bac-cc3456f22013",
   "metadata": {},
   "source": [
    "#### Plotting\n",
    "\n",
    "Of course, another great method for scrutinizing clusters is directly plotting the relationships between the clusters you've built and the data themselves. **The plots are up to you, and depend on your data**, but here we'll look at just the top styles of beer, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00490c2f-b0d3-4375-9218-01d78ecb847e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# top 5 breweries\n",
    "mask = df_beers_top['beer_style'].value_counts().index[:5]\n",
    "mask = df_beers_top['beer_style'].isin(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b322427-503f-4665-8f1a-d418dafc6793",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = sns.histplot(data=df_beers_top[mask].sort_values('cluster_kmeans'),\n",
    "                 x='cluster_kmeans', \n",
    "                 hue='beer_style', palette='Set1', multiple='stack')\n",
    "\n",
    "g.set_title('Distribution of Top Beer Styles Across Clusters')\n",
    "\n",
    "sns.move_legend(g, \"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0007b373-f93b-49df-a31b-af93180a86e1",
   "metadata": {},
   "source": [
    "Here, we can see the some styles of beer are more prevalent in some clusters than others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf53b7ba-3bf5-4d27-991f-c90a02b827f8",
   "metadata": {},
   "source": [
    "## Recommender Systems\n",
    "\n",
    "A recommender system is an automated system that seeks to suggest whether a given item will be desirable to a user. These are algorithms that aim to provide the most relevant items to a user by **filtering useful information from noise.** For example, suppose there are too many books at the bookstore to browse at once. Let's say I’ve bought a few books related to poetry. In this case, a book recommender system might:\n",
    "\n",
    "    1. Log the interest in poetry\n",
    "    2. Review other customers’ interests\n",
    "    3. Reference meta-information on all its books and customers\n",
    "    4. Suggest a set of products\n",
    "    \n",
    "There are two kinds of recommendation:\n",
    "\n",
    "- Content-Based Filtering\n",
    "- Collaborative Filtering\n",
    "\n",
    "We can also combine these two into a sort of \"hybrid\" method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19555858-11f2-4c4f-b327-a55aef2a90d7",
   "metadata": {},
   "source": [
    "### Content-Based Filtering\n",
    "\n",
    "With Content-Based Filtering, we define relevance using only item information. All we’d need is a matrix of items and their attributes. I.e., we project items into their feature space. The recommendation comes from item similarity. In this case, our definition of similarity is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe3e4fe-9588-4e03-8358-b9dcf0adc0cd",
   "metadata": {},
   "source": [
    "#### Calculate Pairwise Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538cc832-f527-4480-a081-36f7fbf3f25e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try metric='euclidean' and metric='cosine', and see what changes!\n",
    "dists = pairwise_distances(df_beers_pca, metric='cosine')\n",
    "dists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f55e85-2fa9-4be0-a535-d270e7285f1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Numpy outputs an array here, so we want to get the names of the beers back\n",
    "dists = pd.DataFrame(data=dists, \n",
    "                     index=df_beers_pca.index, \n",
    "                     columns=df_beers_pca.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2589239e-0439-437f-b304-30b3673fc109",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dists.iloc[0:5, 0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7384ab-7bab-4932-b0ca-7923355437e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Devise \"Ranking\" Scheme\n",
    "\n",
    "**Here is where we can get creative!**\n",
    "\n",
    "Let's say that a user has three beers that they like. We're going to say that a 'likeable' beer is one which is relatively close to all three of these beers. That is, the *sum* of the distances bewteen the 'likeable' beer and the three liked beers is minimal.\n",
    "\n",
    "Start by selecting three beers and store them in `beers_i_like` then look their distances to other beers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ab67f6-ba72-411c-a848-6c7a9fb94aed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "beers_i_like = ['Belgian Blonde (44506)', 'ÜberFest Pilsner (39361)', '\"Cellar Reserve\" Triple Gold (25108)']\n",
    "\n",
    "# view distances across the first 10 rows\n",
    "dists[beers_i_like].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e87188-696a-4297-9476-f9c3bcb3768c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, we sum each of the distances to the favorite beers by row. That is, we are summing 3 numbers for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca9b55a-0343-41b0-922c-da483626c6fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "beers_summed = dists[beers_i_like].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800597c1-46f4-4713-9abc-1875d7df0226",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "beers_summed = beers_summed.sort_values(ascending=True)\n",
    "beers_summed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def5d100-836c-471c-94aa-08dbaebe0d2a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter out the beers used as input using `.isin()`\n",
    "mask = ~beers_summed.index.isin(beers_i_like)\n",
    "ranked_beers = beers_summed.index[mask]\n",
    "ranked_beers = ranked_beers.tolist()\n",
    "\n",
    "ranked_beers[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ae4b7c",
   "metadata": {},
   "source": [
    "Note that the similarity here is not just based on the attributes of reviews (e.g., `beer_type`, `abv`, etc.), but it is based on the \"beer review flavors\" from PCA. In other words, two beers will be similar if they are correlated with the same kinds of \"beer review flavors\" (i.e., the same feature behaviors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f6ba6a-f40d-49cc-b43b-53b998838312",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "One way to evaluate a recommender system such as this one is to compare the features of the preferred items to the ones which have been recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8288a841-ff1b-4a75-ad51-9d7c793091d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "beers_i_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c67937-b601-43a3-8820-f65ac7a18201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# top 5 recommendations\n",
    "recommendations = ranked_beers[:5]\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03f1308-7d86-4439-9edc-7895a922f8b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_eval = df_beers_top.copy()\n",
    "\n",
    "# Use `np.where` to create a categorical variable based on boolean series\n",
    "df_eval['rec_label'] = np.where(df_eval.index.isin(beers_i_like), 'Like',\n",
    "                       np.where(df_eval.index.isin(recommendations), 'Recommended',\n",
    "                       'Other'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e08d79-f25d-4e21-b91a-3c7c9332ed98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_eval[df_eval.rec_label.isin(['Like', 'Recommended'])] \\\n",
    "        .sort_values('rec_label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708998ab-59aa-4d7b-9944-bd34936d4495",
   "metadata": {},
   "source": [
    "In a way, this makes sense. Let's put this into a function, and see how things change with new beers and distance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af02d06e-dbe6-4cf6-9105-d5a0d31ec460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def recommend_beers(beers_i_like, df_pca, num_recs=5, metric='euclidean'):\n",
    "    # calculate pairwise distances\n",
    "    dists = pairwise_distances(df_pca, metric=metric)\n",
    "    dists = pd.DataFrame(data=dists, index=df_pca.index, columns=df_pca.index)\n",
    "    \n",
    "    # ranking scheme\n",
    "    beers_summed = dists[beers_i_like].sum(axis=1)\n",
    "    beers_summed = beers_summed.sort_values(ascending=True)\n",
    "    \n",
    "    # remove preferred beers\n",
    "    ranked_beers = beers_summed.index[~beers_summed.index.isin(beers_i_like)]\n",
    "    ranked_beers = ranked_beers.tolist()\n",
    "    \n",
    "    # get recommendations (top-ranked)\n",
    "    recommendations = ranked_beers[:num_recs]\n",
    "    print(\"Recommendations: \")\n",
    "    print('\\t' + '\\n\\t'.join(recommendations))\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "def rec_eval(beers_i_like, recommendations, df_features):\n",
    "    df_eval = df_features.copy()\n",
    "\n",
    "    df_eval['rec_label'] = np.where(df_eval.index.isin(beers_i_like), 'Like',\n",
    "                                    np.where(df_eval.index.isin(recommendations), 'Recommended',\n",
    "                                             'Other'))\n",
    "    \n",
    "    df_eval = df_eval[df_eval.rec_label.isin(['Like', 'Recommended'])] \\\n",
    "                        .sort_values('rec_label')\n",
    "    \n",
    "    return df_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d174e4b-3a0d-438c-b3db-7b6a26324677",
   "metadata": {},
   "source": [
    "With Euclidean distance ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d07d62f-fe7f-468d-b8ba-da97a324f403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recommendations = recommend_beers(beers_i_like, \n",
    "                                  df_pca=df_beers_pca, \n",
    "                                  metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428e6994-46bf-4e97-bb51-aced3df0af15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rec_eval(beers_i_like, recommendations, df_beers_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16410c49-b504-4c30-ac86-1af258ee4ce5",
   "metadata": {},
   "source": [
    "With Cosine distance ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a180e5-ff40-4bc0-a9a3-2e5750bde265",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recommendations = recommend_beers(beers_i_like,\n",
    "                                  df_pca=df_beers_pca,\n",
    "                                  metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bbc728-1c60-428d-92ef-2481d8d2de5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rec_eval(beers_i_like, recommendations, df_beers_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99ffde8-0203-4680-a073-0901c643e13e",
   "metadata": {},
   "source": [
    "**Caveats of Content-Based Filtering**\n",
    "\n",
    "- Recommendations biased toward past user preference\n",
    "    - Consider the feedback loop …\n",
    "- Diversity of recommendation space is diminished\n",
    "    - We can only work within neighborhoods of past items\n",
    "- Cross domain recommendations are difficult\n",
    "    - Think about recommending movies based on podcasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1739db8a-bb2f-49ae-ae16-b017f6c1e713",
   "metadata": {},
   "source": [
    "### Collaborative\n",
    "\n",
    "Here, we use other users’ history to predict new users’ preference. We define relevance using only user-item relationships, based on a user-item matrix containing `review_overall` ratings as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23c790d-d24d-4a01-bba0-dc98e450bbad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's restrict our dataset to the top reviewers.\n",
    "n = 100\n",
    "\n",
    "# The number of beers reviewed for each user\n",
    "reviewer_counts = df_beer_reviews.groupby('review_profilename')['beer_beerid'] \\\n",
    "                                 .nunique().sort_values(ascending=False)\n",
    "\n",
    "top_n_reviewers = reviewer_counts.index[:n]\n",
    "\n",
    "# this is the data we want\n",
    "df_filtered = df_beer_reviews[df_beer_reviews['review_profilename'] \\\n",
    "                              .isin(top_n_reviewers)].copy()\n",
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ac7af9-5573-41af-b227-2768ada0b307",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_filtered[['beer_beerid', 'beer_name', \n",
    "             'review_profilename', 'review_overall']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9553abc-545e-4cff-8103-c8559742ac03",
   "metadata": {},
   "source": [
    "#### Collect Users as Features\n",
    "\n",
    "What we want is a matrix with users on one axis (e.g., columns), and items on the other axis (e.g., rows). Here, we start by grouping our data to get an aggregate reprsentative value for each item-user combination, and then use `unstack` to widen the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ffa5de-8e8b-43e7-92d9-09fd576a5147",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a useful index\n",
    "df_filtered['beer_id'] = df_filtered['beer_name'] + ' (' \\\n",
    "    + df_filtered['beer_beerid'].astype(str) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085abd06-1054-4543-8fc7-b46804ab9ba1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For each beer (row), what is the average review score for each user (column)\n",
    "df_beer_user = (df_filtered\n",
    "                .groupby([\"beer_id\", \"review_profilename\"])['review_overall']\n",
    "                .mean())\n",
    "df_beer_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d54da-29bb-43c9-9937-e4400d677c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert to a wide version\n",
    "df_beer_user = df_beer_user.unstack()\n",
    "\n",
    "df_beer_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efdb8a8-055c-435a-bb70-9e3088eebd93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_beer_user.isna().sum() / df_beer_user.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ca4f65-a03f-4520-9bdb-61eddac1ece1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**How should we handle missing values here?** What does it mean for a beer *not* to be reviewed by a user? Should we treat these the same as the situations where a user reviewed a beer, and rated it zero? What if we used a negative number, how would that affect our distances/similarities between beer-user entities? What if we used the average value for each column, can we assume that a beer unreviewed by user $x$ will have their average beer rating?\n",
    "\n",
    "What if we assume (for the time being) that an unreviewed beer has an (impossible) rating of -1. This way, we can capture the fact that user hasn't reviewed that beer. It also comes in handy later when trying to spot the beers that have been rated vs. those which have not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b5e8aa-2e0c-4d62-b6db-c3d7cd1abece",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_beer_user.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7ec909-cd00-4791-bac6-62dc7d5eec67",
   "metadata": {},
   "source": [
    "Recall that Singular Value Decomposition (SVD) is a method we can use to understand latent \"topic directions\" in text data. So, given a document-term matrix, we can get an idea for the kinds of topics that exist in the data (i.e., the ways words are correlated across documents). With our user-item data, we can use SVD to understand sort of \"*usage* directions\". In this way, given a user-item matrix, we get an idea for the ways items are correlated across users given our data set.\n",
    "\n",
    "<center>\n",
    "    <img width='65%' src=\"https://docs.google.com/drawings/d/e/2PACX-1vQ2EGkUO7gDlYxhzqfFudz4MeFAtefwahuML0Gu3mRQ-oNrMK9hGkA3a-Dhkq5Sx5zrbjuJen-7_7iP/pub?w=960&h=720\">\n",
    "</center>\n",
    "\n",
    "Traditionally, we'll want to reduce our number of *usage directions* (i.e., the number of principal components we consider) to some number less than the total number of users in our data set. This way, we can look at our data from the perspective of usage (the directions) and weights with which they're associated (how strongly a user/item is correlated in a direction). *In fact, we can use these weights to assign users (or items, for that matter) to groups.*\n",
    "\n",
    "[SciPy](https://docs.scipy.org/doc/scipy/index.html) is another Python library we will see more of in a later lesson. It's built to manage all kinds of mathematical operations such as linear algebra, signal processing, etc. Here, we'll use it's implementation of SVD so that we can capture all three of these matrices $U$, $\\Sigma$, and $V$, since SKLearn's `PCA` only gives us the first two matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd8a306-471e-4b65-b27b-3d964d86e09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a3171b-aa41-4bde-99d3-5fa46f2b785d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "beer_usage, usage, usage_user = svd(df_beer_user, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f220d931-1ed7-4bda-a73d-1d2f4928c3f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For each beer, how much of their use (rating) is captured by each usage type?\n",
    "df_beer_usage = pd.DataFrame(data=beer_usage, index=df_beer_user.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a405af-3d94-48b6-bb59-ac9b00f37082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For each user, how much of their activity is captured by each usage (rating scheme) type?\n",
    "df_usage_user = pd.DataFrame(data=usage_user, columns=df_beer_user.columns)\n",
    "df_user_usage = df_usage_user.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03346a9c-b02a-4bf6-ac16-2e3a01d820fb",
   "metadata": {},
   "source": [
    "- `df_beer_usage` is now a matrix where each row represents a beer, and each column represents a kind of *usage*. If we look at any row (beer), the columns tell us how much information about that beer can be gathered solely by looking at it's usage along each usage type\n",
    "- `df_user_usage` is a matrix where each row is a kind of usage, and the columns are users. Similar to the above, it tells us how much information about that user can be gathered just by looking at their activity in some usage type.\n",
    "- `usages` contains the sort of variety of users/beers that are represented by each usage type. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68508588-6a9c-4473-89c1-4719d1903b98",
   "metadata": {},
   "source": [
    "#### Similar User Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c94f25-4add-4e81-a6a2-d9db72917ba8",
   "metadata": {},
   "source": [
    "Recall that the `usages` came from `svd`, and it represents the singular values (the *weights*) of the Singular Value Decomposition. Each singular value tells us *how much* a principal component (\"usage type\") tells us about the user or item. So, naturally, if we take any singular value `usage[i]` and divide it by the sum of all the usages `usage[i] / usage.sum()`, we have the amount of \"explained variance\" that the `i`th singular value provides. \n",
    "\n",
    "Also, `usages` is **naturally sorted** in descending order by the `svd` algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cf5448-7574-494b-8e7a-9e908d8e9dc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the explained variance of each of these usage types\n",
    "exp_var_ratios = usage / usage.sum()\n",
    "\n",
    "g = plot_pca(exp_var_ratios, threshold = 0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c861aef9-2192-4913-a90e-7c09b0d2d5ed",
   "metadata": {},
   "source": [
    "Notice that it takes many more principle components to capture 85% of the variance in this data. In other words, there are about 80 different ways to review beer that are relatively (i.e., \"85%\") distinct from one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2c91b5-beb3-42fc-b956-ce67abbea767",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To keep 85% of the variance, let's consider the first 80 usage types (principal components)\n",
    "dists_users = pairwise_distances(df_user_usage.iloc[:, :80], metric='euclidean')\n",
    "dists_users = pd.DataFrame(dists_users, index=df_user_usage.index, \n",
    "                           columns=df_user_usage.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cc335e-d746-4e02-b3da-1ab0c09b1b7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_user_neighbors(user, neighborhood=10):\n",
    "    '''\n",
    "    Given a user, return the closest users by the `dists_users` matrix\n",
    "    '''\n",
    "    neighbors = dists_users[user].sort_values()[:neighborhood]\n",
    "    \n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eef87f",
   "metadata": {},
   "source": [
    "Consider a singular user, and their closest neighbors when we consider their \"usage\" type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a72d26e-b37f-4d26-9644-a91474a40239",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_user_usage.index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c483bd33-77d6-458d-810b-a53771506f8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for example\n",
    "neighbors = get_user_neighbors('BEERchitect')\n",
    "neighbors.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad08f286-27aa-4090-a422-1c1323525375",
   "metadata": {},
   "source": [
    "**Active Reviewers**\n",
    "\n",
    "Recall that we encoded a negative rating to mean that the user did not review that beer. So, we'll gather the \"active\" reviewers by capturing the rows with the most non-negative values, and sorting them in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64864892-c956-4880-b37c-30319845c06f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# count non-negative values (i.e., beers with reviews), and sort\n",
    "top_users = (df_beer_user > 0).sum(axis=0) \\\n",
    "                              .sort_values(ascending=False).index[:10]\n",
    "top_users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fb4ed0-9ed3-46b2-8723-ebea704948ad",
   "metadata": {},
   "source": [
    "**Check**\n",
    "\n",
    "Let's create a plot with the known ratings of the user in question along with ratings of the same beers given by the neighbors we calculated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224fcaeb-d7cb-4177-a582-4d2f3231510f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_usage(user, neighborhood=10):\n",
    "    # Get the `neighborhood` closest neighbors\n",
    "    neighbors = get_user_neighbors(user, neighborhood).index.tolist()\n",
    "    \n",
    "    # build array of review values for beers reviewed by neighbors\n",
    "    mask = df_filtered['review_profilename'].isin(neighbors)\n",
    "    df_plot = df_filtered[mask][['review_profilename', \n",
    "                                 'beer_style', \n",
    "                                 'review_overall']]\n",
    "    df_plot = (\n",
    "        df_plot\n",
    "        .groupby(['beer_style', 'review_profilename'])['review_overall']\n",
    "        .mean().unstack())\n",
    "\n",
    "    df_plot = df_plot.loc[:, neighbors] \\\n",
    "                     .sort_values(user, ascending=False) \\\n",
    "                     .dropna(how='all')\n",
    "    \n",
    "    # We'll only annotate the heatmap if the text is easier to see\n",
    "    df_annot = df_plot.round(2) if df_plot.shape[0] <= 20 else None\n",
    "    \n",
    "    # We're sure to be intentional about a \"center\" point\n",
    "    plt.title(f\"Top Ratings for Neighbors of {user}\")\n",
    "    sns.heatmap(df_plot,\n",
    "                annot=df_annot,\n",
    "                cmap='coolwarm_r',\n",
    "                center=3);  # ratings range from 1-5, and 3 is center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078e7d23-acc5-4a9d-95ad-75159cd7d0f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_usage('mikesgroove')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2803ade-c48d-4518-9864-57546ac64b13",
   "metadata": {},
   "source": [
    "Based off this, could we conclude this is a good recommender?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795b2dd8-c87d-44b8-8093-9c986a4a3e58",
   "metadata": {},
   "source": [
    "#### Recommendation\n",
    "\n",
    "Say for some user, we take the top `m` closest users, and recommend `r` different beers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bde74da-81c2-4739-8344-2534c45aaeb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user = 'dyan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7237f0-d4c5-4f7f-b659-62776e94f8ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We consider the 20 closest users (reviewers) to choose untried beers\n",
    "n_neighbors = 20\n",
    "\n",
    "neighbors = get_user_neighbors(user, n_neighbors+1)\n",
    "neighbors = neighbors[1:]  # We don't want to include the user themself\n",
    "neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530400d5-c77b-491d-a9ee-5eac475a26cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Look at *untried* beers among the neighboring users\n",
    "untried_beers = df_beer_user[user] < 0\n",
    "df_rec = df_beer_user.loc[untried_beers, neighbors.index].copy()\n",
    "\n",
    "# Replace the un-rated beers with NAN values\n",
    "df_rec = df_rec.replace(-1, np.nan)  \n",
    "\n",
    "# Only keep rows where at least one neighbor rated the beer\n",
    "df_rec = df_rec.dropna(how='all')\n",
    "\n",
    "df_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8504a8a-fefe-47a2-bdbe-89872b72e7b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the average rating among the top `n_neighbors` of our user (ignoring NaNs)\n",
    "# (HOW COULD WE SMARTER ABOUT THIS?)\n",
    "avg_beer_rating = np.nanmean(df_rec, axis=1)\n",
    "avg_beer_rating = pd.Series(index=df_rec.index, data=avg_beer_rating)\n",
    "avg_beer_rating.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "# Return up to `n_recommendations`\n",
    "n_recommendations = 5\n",
    "avg_beer_rating[:n_recommendations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d5f830-5c1a-4eb0-a896-c9821cc17758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Put all this into a function\n",
    "def get_recommendations(user, n_recommendations=5, n_neighbors=20):\n",
    "    neighbors = get_user_neighbors(user, n_neighbors+1)\n",
    "    neighbors = neighbors[1:]  # We don't want to include the user themselves\n",
    "    \n",
    "    untried_beers = df_beer_user[user] < 0\n",
    "    df_rec = df_beer_user.loc[untried_beers, neighbors.index].copy()\n",
    "\n",
    "    df_rec = df_rec.replace(-1, np.nan)  # Replace the un-rated beers with NAN values\n",
    "    df_rec = df_rec.dropna(how='all')  # Only keep rows where at least one neighbor rated the beer\n",
    "    \n",
    "    # Get the average rating among the top `n_neighbors` of our user\n",
    "    avg_beer_rating = np.nanmean(df_rec, axis=1)\n",
    "    avg_beer_rating = pd.Series(index=df_rec.index, data=avg_beer_rating)\n",
    "    avg_beer_rating.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "    # Return up to `n_recommendations`\n",
    "    return avg_beer_rating[:n_recommendations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661b04d8-14bb-4fa3-80c0-459309b2aa55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user = 'Thorpe429'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3817417f-f0d8-4f03-8093-214a7a2045fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_recommendations(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d83a6f-7016-44e3-9898-2db488cd9944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fc72139",
   "metadata": {},
   "source": [
    "# Explore\n",
    "\n",
    "Test your understanding of this week's content with the following explorations.\n",
    "\n",
    "*Note: unless otherwise noted, **explorations are completely optional and will not be reviewed.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b530a699-ec24-487a-948e-f33802dedaa6",
   "metadata": {},
   "source": [
    "## Exploration 1\n",
    "\n",
    "Write a function called `euclidean` which calculates the Euclidean distance between two points (of any `k` dimensions), using only vanilla Python (no packages). This should return a single value.\n",
    "\n",
    "**(Optional)** Adjust your function to calculate *pairwise* Euclidean distances for any number of points (i.e., return a matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bad90a-81b3-4e23-ad67-3dba5a075374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ece0b9be-4978-48bd-87ec-ddf48895669d",
   "metadata": {},
   "source": [
    "## Exploration 2\n",
    "\n",
    "Let `r` be some value **less** than `df_user_usage.shape[1]`. For example, the `r` above corresponds to roughly 85% of the variance in the user data.\n",
    "\n",
    "Use your function from Exercise 1 and the `df_user_usage.iloc[:, :r]` dataframe to calculate the distance between any two users. Interpret your results for a random selection of two users compared to another selection of two *other* users.\n",
    "\n",
    "\n",
    "#### Bonus\n",
    "\n",
    "*Note: This requires some mathematical understanding of SVD.*\n",
    "\n",
    "Set `r = None`. Given the fact that `df_user_usage` came **after** PCA, explain the distances calculated in Exercise 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc611981-d4d6-4ca5-a656-581d7dcc9d08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "h501-week-11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
